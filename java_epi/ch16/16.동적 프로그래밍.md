# 16장 동적 프로그래밍

동적 프로그래밍은 부분 문제로 분해할 수 있는 최적화, 탐색, 계산 문제를 해결하기 위한 일반적인 방법을 말한다. 해법에 도달하기 위해 선택을 해야할 때마다 (특히 해법이 부분 문제와 관련이 있는 경우) 동적 프로그래밍을 고려해 봐야 한다.

분할 정복법처럼 동적 프로그래밍은 작은 문제 여러 개를 합쳐서 큰 문제를 풀 수 있다. 하지만 분할 정복과는 다르게, 동적 프로그래밍에서는 동일한 하위 문제가 반복해서 발생할 수 있다. 따라서 동적 프로그래밍을 효율적으로 작성하려면 반복해서 발생하는 하위 문제의 결괏값을 캐싱(caching)해 두어야 한다. 면접에서는 어려운 문제를 출제하려고 할 때, 동적 프로그래밍 문제를 선택하는 경향이 있다.

우선 피보나치 수열을 구하는 문제를 통해 동적 프로그래밍의 기본 개념을 살펴보자. 피보나치 수열은 0,1로 시작한다. 그 다음 숫자는 이전 숫자 두 개의 합이다. 따라서 0,1,1,2,3,5,8,13,21,...이 된다. 이 수열은 생물학, 자료구조 분석, 병렬 컴퓨팅 등 다양한 애플리케이션에서 활용된다.

수학적으로 표현하자면 n번째 피보나치 수열은 $F(n) = F(n-1) + F(n-2)$이고, $F(0) = 0, F(1) = 1$이다. 이때 $F(n)$을 재귀적으로 계산하는 함수의 시간 복잡도는 기하급수적으로 증가한다. 왜냐하면 재귀 함수가 F(i)를 반복적으로 호출하기 때문이다. 그림 16.1을 보면 같은 함수가 어떻게 반복적으로 호출되는지 알 수 있다.

중간 결과를 캐시에 저장했을 때 n번째 피보나치 수열을 구하는 시간 복잡도는 n에 선형적으로 비례하지만, 추가적인 공간 복잡도는 $O(n)$이 된다.

```java
private static Map<Integer, Integer> cache = new HashMap<>();

public static int fibonacci(int n) {
    if (n <= 1) {
        return n;
    } else if (!cache.containsKey(n)) {
        cache.put(n, fibonacci(n - 2) + fibonacci(n - 1));
    }
    return cache.get(n);
}
```

캐시 공간을 최소화하는 것은 동적 프로그래밍에서 중요한 이슈이다. 이번에는 같은 프로그램을 $O(n)$ 시간과 $O(1)$ 공간에 구해 보자. 다음 코드는 앞에서 살펴본 프로그램과 개념적으로는 반대다. 상향식 방식으로 캐시를 반복적으로 채워 공간 복잡성을 줄이고, 캐시를 재사용할 수 있게 한다.

```java
public static int fibonacci(int n) {
    if (n <= 1) {
        return n;
    }

    int fMinus2 = 0;
    int fMinus1 = 1;
    for (int i = 2; i <= n; i++) {
        int f = fMinus2 + fMinus1;
        fMinus2 = fMinus1;
        fMinus1 = f;
    }
    return fMinus1;
}
```

동적 프로그래밍을 효율적으로 푸는 핵심은 하나의 문제를 부분 문제로 나누는 방법을 찾는다. 부분 문제는 다음과 같은 특징이 있다. 이 특징을 잘 이용하면 부분 문제를 쉽게 찾을 수 있다.

- 부분 문제의 해법을 찾으면 원래 문제도 비교적 쉽게 해결할 수 있다.
- 부분 문제의 해법을 캐시에 저장할 수 있다.

이번에는 좀 더 복잡한 문제를 다루어 보자. 정수 배열이 주어졌을 때 합이 가장 큰 부분배열을 동적 프로그래밍으로 구하라. 그림 16.2의 배열이 주어졌을 때 합이 최대가 되는 부분배열은 0번 인덱스에서 3번 인덱스까지다.

```text
    904, 40, 523, 12, -335, -385, -124, 481, -31
    A[0],A[1],A[2],A[3],A[4],A[5],A[6],A[7],A[8]
```

모든 부분배열의 합을 구해야 하므로 무식하게 풀면 시간 복잡도가 $O(n^3)$이 된다. 부분배열의 개수는 $\frac{n(n-1)}{2}$이고, 각 부분배열의 합을 구하는 데 $O(n)$의 시간이 걸리기 때문이다. 하지만 추가로 $O(n)$의 공간을 사용해서 모든 k에 대한 $S[k] = \sum{A[0,k]}$를 저장할 수 있다면, 무식한 방법의 시간 복잡도를 $O(n^2)$으로 개선할 수 있다. $A[i,j]$의 합은 $S[j] - S[i - 1]$이고, $S[-1]$은 0이 된다.

분할 정복법으로도 이 문제를 풀 수 있다. 먼저 A의 중간 인덱스인 $m = \lfloor\frac{n}{2}\rfloor$을 구한다. 부분배열 $L = A[0,m]$과 $R = A[m + 1, n - 1]$에 대해서 같은 문제를 푼다. 각각의 최댓값을 구하는 것 외에도 L 배열의 인덱스 m에서 끝나는 부분배열의 최대합 l과 R 배열의 첫 인덱스에서 시작하는 부분배열의 최대합 r 또한 구해야 한다. A의 최대 부부배열합은 l + r, L의 최대합, R의 최대합 중에 큰 값이 된다. 시간 복잡도는 $O(n log n)$이며, 퀵정렬의 시간 복잡도를 분석하는 방법과 비슷하게 구할 수 있다. 인덱스를 다루는 작업이 까다로워 프로그램을 올바르게 작성하는 데 시간이 걸릴 것이다.

이제 동적 프로그래밍을 사용해서 문제를 풀어 보자. 부분배열 $A[0, n-2]$의 해법을 알고 있다고 가정하고 문제를 푸는 것에서 시작하고 싶을 것이다. 그런데 부분배열 $A[0, n-2]$의 최대부분합을 알고 있다고 하더라도 $A[0, n-1]$의 해법을 구하는 데 도움이 되지 않는다. i < n - 1의 모든 부분배열 A[0,i]에 대해서 가장 작은 부분배열의 합을 알고 있어야 도움이 된다. 이 정보를 알고 있으면 S[n-1]에서 부분배열의 합을 뺀 값 중에 최댓값이 정답이 되기 때문이다. 다시 말해 j로 끝나는 최대부분합은 $S[j] - \min_{k \le j}S[k]$가 된다. 배열을 차례대로 순회할 때마다 합이 가장 작은 S[k]를 추적할 수 있으므로 각 인덱스에 대한 최대 부분합을 찾을 수 있다. 각 인덱스에서 최대 부분합을 구하는 데 상수 시간이 걸리므로 시간 복잡도는 $O(n)$이고 공간 복잡도는 $O(1)$이 된다. 모든 배열의 값이 음수일 수도 있고 배열이 비어 있을 수도 있기 때문에, 이런 경우의 입력도 처리할 수 있어야 한다.

```java
public static int findMaximumSubarray(List<Integer> A) {
    int minSum = 0, sum = 0 maxSum = 0;
    for (int i = 0; i < A.size(); i++) {
        sum += A.get(i);
        if (sum < minSum) {
            minSum = sum;
        }
        if (sum - minSum > maxSum) {
            maxSum = sum - minSum;
        }
    }
    return maxSum;
}
```

## 동적 프로그래밍 부트 캠프

방금까지 살펴본 피보나치 수열과 최대 부분합을 구하는 문제는 동적 프로그래밍을 연습하기에 좋은 예제다.

- 문제가 부분 문제와 관련이 있는 경우에는 더더욱 동적 프로그래밍을 고려해 봐야 한다.
- 동적 프로그래밍은 최적화 문제뿐 아니라 **개수를 세는 문제, 의사 결정 문제**를 푸는 데도 쓸 수 있다. 동일한 계산을 재귀적으로 사용하는 작은 부분 문제들로 사용하는 작은 부분 문제들로 전체 문제를 표현할 수 있다면, 동적 프로그래밍으로 풀 수 있다.
- 이와 같은 개념으로 보자면, 동적 프로그래밍에는 재귀가 포함된다. 하지만 캐시는 효율성을 위해 **상향식**, 즉 반복적으로 구축되는 경우가 많다. [문제 16.3]
- 동적 프로그래밍이 재귀적으로 구현될 때, 캐시는 보통 해시 테이블이나 이진 탐색 트리 같은 동적 자료구조로 구현된다. 반복적으로 구현되는 캐시는 보통 1차원이나 다차원 배열이다.
- 공간을 절약하기 위해 다시 사용하지 않게 될 **캐시 공간**을 **재사용**할 수도 있다. [문제 16.1, 16.2]
- 가끔은 **상향식 동적 프로그래밍 해법보다 재귀가 더 효율적**일 때가 있다. 예를 들어 해법을 빨리 찾을 수 있거나 **가지치기**를 통해 탐색해야 할 부분 문제의 개수를 줄이는 경우다. [문제 16.5]
- 동적 프로그래밍은 부분 문제에 대한 **해법을 결합**하여 원래 문제에 대한 **해법을 제시**한다. 하지만 동적 프로그래밍으로 풀 수 없는 문제도 있다. 예를 들어 중간 도시를 반복해서 경유하지 않도록, 도시 1에서 도시 2까지 가는 가장 긴 경로를 구해야 한다고 하자. 이 경로는 도시 3을 통과한다. 이때 도시 1에서 도시 3으로, 그리고 도시 3에서 도시 2로 가는 각각의 하위 경로는, 문제에서 요구했던 '중간 도시를 반복해서 경유하지 않는 가장 긴 경로'가 아닐 수도 있다.
